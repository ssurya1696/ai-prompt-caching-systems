{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b01f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0,api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd39a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    vector = embedding_model.embed_query(text)\n",
    "    return np.array(vector).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7047896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00692033, -0.03531143,  0.00159792, ..., -0.01301259,\n",
       "        -0.01907527, -0.00606268]], shape=(1, 1536))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beab7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticPromptCache:\n",
    "    def __init__(self, similarity_threshold=0.9):\n",
    "        self.cache = []\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.metrics = {\n",
    "            \"hits\": 0,\n",
    "            \"misses\": 0,\n",
    "            \"total_requests\": 0\n",
    "        }\n",
    "\n",
    "    def get(self, prompt: str):\n",
    "        self.metrics[\"total_requests\"] += 1\n",
    "        new_embedding = get_embedding(prompt)\n",
    "\n",
    "        for entry in self.cache:\n",
    "            similarity = cosine_similarity(\n",
    "                new_embedding,\n",
    "                entry[\"embedding\"]\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                self.metrics[\"hits\"] += 1\n",
    "                print(f\"Semantic Cache HIT (similarity={round(similarity,2)})\")\n",
    "                return entry[\"response\"]\n",
    "\n",
    "        self.metrics[\"misses\"] += 1\n",
    "        print(\"Semantic Cache MISS\")\n",
    "        return None\n",
    "\n",
    "    def set(self, prompt: str, response: str):\n",
    "        embedding = get_embedding(prompt)\n",
    "\n",
    "        self.cache.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"embedding\": embedding,\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "    def stats(self):\n",
    "        hit_rate = (\n",
    "            self.metrics[\"hits\"] / self.metrics[\"total_requests\"]\n",
    "            if self.metrics[\"total_requests\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            **self.metrics,\n",
    "            \"hit_rate\": round(hit_rate, 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65eb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_cache = SemanticPromptCache(similarity_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d45def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_cached_llm_call(prompt: str):\n",
    "\n",
    "    cached_response = semantic_cache.get(prompt)\n",
    "\n",
    "    if cached_response:\n",
    "        return cached_response\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    output = response.content\n",
    "\n",
    "    semantic_cache.set(prompt, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5b9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(prompt: str):\n",
    "\n",
    "    start = time.time()\n",
    "    result = semantic_cached_llm_call(prompt)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Latency: {round(end - start, 3)} seconds\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875d2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Call (MISS)\n",
      "Semantic Cache MISS\n",
      "Latency: 6.734 seconds\n",
      "\n",
      "Second Call (Should HIT if similar)\n",
      "Semantic Cache HIT (similarity=0.95)\n",
      "Latency: 0.221 seconds\n",
      "\n",
      "Cache Stats:\n",
      "{'hits': 1, 'misses': 1, 'total_requests': 2, 'hit_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Explain AI in two sentence\"\n",
    "prompt2 = \"Describe AI in two sentence\"\n",
    "\n",
    "print(\"First Call (MISS)\")\n",
    "measure_latency(prompt1)\n",
    "\n",
    "print(\"Second Call (Should HIT if similar)\")\n",
    "measure_latency(prompt2)\n",
    "\n",
    "print(\"Cache Stats:\")\n",
    "print(semantic_cache.stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
